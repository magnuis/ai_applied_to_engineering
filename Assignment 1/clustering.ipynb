{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conlusions\n",
    "### Clustering\n",
    "The KMeans clustering with three clusters appears to more or less have taken one of the clusters obtained with `n_components=2`, and split it. This caused difficulties in separating the two new clusters, and did not provide useful insights. We therefore focused on two clusters for obtaining customer profiles. \n",
    "### Separating features\n",
    "The features that separate the clusters are `Kidhome`, `MntWines`, `NumStorePurchases`, `MntFishMeatProdcts`.\n",
    "### Customer profiles\n",
    "|Customer group 0|Customer group 1|\n",
    "|----------------|----------------|\n",
    "|Has kids        |No kids         |\n",
    "|Fewer purchases*|More purchases, both online and in store**|\n",
    "|Spend more across all categories***|Spend less|\n",
    "\n",
    "*Despite having fewer web purchases, group 0 visit the web more often.\n",
    "**In addition to shopping more often, group 1 has higher average spend.\n",
    "***Most prominent differences in wine and meat/fish.\n",
    "\n",
    "### Additional insights\n",
    "Those who accept more campaigns (>3)\n",
    "- are more educated\n",
    "- has fewer kids\n",
    "- spend a lot on wine, but not much in the other categories\n",
    "- make more purchases in the store than the web\n",
    "\n",
    "\n",
    "### PCA \n",
    "Visualization with PCA strongly suggested three non-spherical clusters. As expected, the clustering from KMeans did not align with the clustering proposed by PCA.\n",
    "### t-SNE\n",
    "Visualization with t-SNE suggested four to six clusters. The clustering from KMeans did not align with the clustering proposed by t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering using k-means\n",
    "\n",
    "This notebook shows the clustering part of the assignment. For simplicity, it uses the custom `Preprocessor` class for data preparation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocessor import Preprocessor\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "prep = Preprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by looking at whether the clustering is better with or without the new features, and simultaneously finding a suggestion for the optimal number of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code retrieved and modified from example notebook\n",
    "\n",
    "def get_metrics(df):\n",
    "       sse = []\n",
    "       silhouettes = []\n",
    "       for i in range(2, 11):\n",
    "              km = KMeans(n_clusters=i,random_state=0)\n",
    "              labels = km.fit_predict(df)\n",
    "              sse.append(km.inertia_)\n",
    "              silhouettes.append(silhouette_score(df, labels))\n",
    "       return sse, silhouettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with, _ = prep.get_data(test_size=-1, remove_outliers=True, aggregate_features=True)\n",
    "sse_with, silhouette_with = get_metrics(df_with)\n",
    "\n",
    "df_without, _ = prep.get_data(test_size=-1, remove_outliers=True, aggregate_features=False)\n",
    "sse_without, silhouette_without = get_metrics(df_without)\n",
    "\n",
    "sns.lineplot(x=range(2,11),y=sse_with,marker='o', label='With aggregated features');\n",
    "sns.lineplot(x=range(2,11),y=sse_without,marker='o', label='Without aggregated features');\n",
    "plt.xlabel('Number of clusters') \n",
    "plt.ylabel('Sum of Squares Within');\n",
    "\n",
    "xpoints = [10,2]\n",
    "ypoints = [sse_with[-1],sse_with[0]]\n",
    "# sns.lineplot(x=xpoints,y=ypoints);\n",
    "plt.show()\n",
    "\n",
    "# Plot silhouette score\n",
    "sns.lineplot(x=range(2,11),y=silhouette_with,marker='o', label='With aggregated features');\n",
    "sns.lineplot(x=range(2,11),y=silhouette_without,marker='o', label='Without aggregated features');\n",
    "plt.xlabel('Number of clusters') \n",
    "plt.ylabel('Silhouette score');\n",
    "\n",
    "xpoints = [10,2]\n",
    "ypoints_with = [silhouette_with[-1],silhouette_with[0]]\n",
    "# sns.lineplot(x=xpoints,y=ypoints_with);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kmeans clustering has consistently lower SSE without aggregated features than with, suggesting stronger cohesion without the aggregated features. The silhouette score is lower for k=2, higher for k=3, and effectively similar for k>=4. We will therefore not aggregate new features prior to the clustering, but rather generate them afterwards to find patterns in the dataset. \n",
    "\n",
    "### Optimal K\n",
    "\n",
    "- The elbow method plotting SSE (wihtout aggregated features) suggests k=3 or k=6 as the optimal number of neighbors. \n",
    "- The silhoutte score plotting (without aggregated features) shows a significant decrease from 2 to 3, and further from 3 to 4. \n",
    "\n",
    "The poor clustering for k>3, combined with the observations for the SSE, tells us to look into k=2 and k=3 for the optimal number of clusters.\n",
    "\n",
    "We proceed by looking at how the clusters differ from each other, by plotting a parallel coordinate plot and calculating the standard deviation of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods to be used for both k=2 and k=3\n",
    "\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# Parallel coordinates plot\n",
    "def plot_parallel(km, df, k):\n",
    "    \n",
    "    data = df.copy()\n",
    "    \n",
    "    # Extract the cluster centers\n",
    "    centers = km.cluster_centers_\n",
    "\n",
    "    # Create a DataFrame from the cluster centers\n",
    "    centers_df = pd.DataFrame(centers, columns=data.columns)\n",
    "    centers_df['Cluster'] = [f'Cluster {i}' for i in range(k)]\n",
    "\n",
    "    # Plot the parallel coordinates\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    parallel_coordinates(centers_df, 'Cluster', color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "    plt.title('Parallel Coordinates Plot of Cluster Centers')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Standard deviations bar chart\n",
    "def plot_std(km, df, k):\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Extract the cluster centers\n",
    "    centers = km.cluster_centers_\n",
    "\n",
    "    # Create a DataFrame from the cluster centers\n",
    "    centers_df = pd.DataFrame(centers, columns=data.columns)\n",
    "\n",
    "    # Compute the standard deviation for each variable across the cluster centers\n",
    "    std_dev = centers_df.std()\n",
    "    # Create a bar plot for the standard deviations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    std_dev.plot(kind='bar', color='skyblue')\n",
    "    plt.title('Standard Deviation of Cluster Center Values Across Variables')\n",
    "    plt.xlabel('Variables')\n",
    "    plt.ylabel('Standard Deviation')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', which='both', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Comparisons scatter plot\n",
    "def plot_comparisons(df):\n",
    "    color_map = {0: 'red', 1: 'blue', 2: 'green'}\n",
    "\n",
    "    # Aggregate new features\n",
    "    data = df.copy()\n",
    "    data = Preprocessor.aggregate_features(data)\n",
    "\n",
    "    feature_pairs = [\n",
    "        ('Kidhome','MntWines'),\n",
    "        ('Kidhome','MntGoldProds'),\n",
    "        ('Kidhome','MntFishMeatProdcts'),\n",
    "        ('Kidhome','MntFruits'),\n",
    "        ('Kidhome','MntSweetProducts'),\n",
    "        ('Kidhome','NumWebPurchases'),\n",
    "        ('Kidhome','NumStorePurchases'),\n",
    "        ('Kidhome','NumWebVisitsMonth'),\n",
    "        ('Kidhome','Recency'),\n",
    "        ('MntWines','MntFishMeatProdcts'),\n",
    "        ('MntWines','MntFruits'),\n",
    "        ('MntWines','MntSweetProducts'),\n",
    "        ('MntWines','MntGoldProds'),\n",
    "        ('NumWebPurchases','NumStorePurchases'),\n",
    "        ('NumWebPurchases','NumWebVisitsMonth'),\n",
    "        ('Num_AcceptedCmp', 'NumStorePurchases'),\n",
    "        ('Num_AcceptedCmp', 'NumWebPurchases'),\n",
    "        ('Num_AcceptedCmp','MntFishMeatProdcts'),\n",
    "        ('Num_AcceptedCmp','MntFruits'),\n",
    "        ('Num_AcceptedCmp','MntSweetProducts'),\n",
    "        ('Num_AcceptedCmp','MntGoldProds'),\n",
    "        ('Num_AcceptedCmp','MntWines'),\n",
    "        ('Num_AcceptedCmp','Years_customer'),\n",
    "        ('Num_AcceptedCmp','Age'),\n",
    "        ('Num_AcceptedCmp','Education'),\n",
    "        ('Num_AcceptedCmp','Kidhome'),\n",
    "        ('TotalSpend', 'AvgSpendPerPurchase'),\n",
    "        ('TotalSpend', 'TotalPurchases'),\n",
    "        ('Num_AcceptedCmp', 'TotalSpend'),\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Determine layout size\n",
    "    n_plots = len(feature_pairs)\n",
    "    n_cols = 2 \n",
    "    n_rows = (n_plots + 1) // n_cols \n",
    "\n",
    "    # Create scatter plots for each combination of features\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(12, 4 * n_rows))\n",
    "\n",
    "    # Flatten the axis array for easier indexing\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, combination in enumerate(feature_pairs):\n",
    "        feat1 = combination[0]\n",
    "        feat2 = combination[1]\n",
    "        axs[i].scatter(data[feat1], data[feat2], c=data['Cluster'].map(color_map), alpha=0.5)\n",
    "        axs[i].set_xlabel(feat1)\n",
    "        axs[i].set_ylabel(feat2)\n",
    "        axs[i].set_title(f'{feat1} vs {feat2}')\n",
    "\n",
    "    # Turn off any unused subplots\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        axs[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_histograms(df):\n",
    "    color_map = {0: 'red', 1: 'blue', 2: 'green'}\n",
    "\n",
    "    # Aggregate new features\n",
    "    data = df.copy()\n",
    "    data = Preprocessor.aggregate_features(data)\n",
    "\n",
    "    # Iterating over features\n",
    "    for feature in data.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Iterating over clusters per feature\n",
    "        for cluster in sorted(data['Cluster'].unique()):\n",
    "            subset = data[data['Cluster'] == cluster]\n",
    "            plt.hist(subset[feature], bins=30, alpha=0.5, label=f'Cluster {cluster}', color=color_map[cluster])\n",
    "\n",
    "        plt.title(f'Distribution of {feature} Across Clusters')\n",
    "        plt.xlabel(f'{feature}')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend(title='Cluster')\n",
    "        plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two clusters\n",
    "\n",
    "### Differences between the two clusters\n",
    "\n",
    "From the parallel coordinates plot and the standard deviations we see that in general the features can be categorized in features that are similar between the clusters (not defining), features that differs greatly (defining), and features that can be deemed neutral:\n",
    "\n",
    "**Defining**\n",
    "- `Kidhome`\n",
    "- `MntWines`\n",
    "- `NumStorePurchases`\n",
    "- `MntFishMeatProdcts`\n",
    "\n",
    "**Neutral**\n",
    "- `MntFruits`\n",
    "- `MntSweetProducts`\n",
    "- `MntGoldProducts`\n",
    "- `NumWebPurchases`\n",
    "- `Age`\n",
    "\n",
    "**Not defining**\n",
    "- `Education`\n",
    "- `Recency`\n",
    "- `NumWebVisitsMonth`\n",
    "- `Years_customer`\n",
    "- `Num_AcceptedCmp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2, _ = prep.get_data(test_size=-1, remove_outliers=True, aggregate_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_2 = KMeans(2)\n",
    "km_2.fit(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parallel(km_2, data_2, 2)\n",
    "plot_std(km_2, data_2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the clusters\n",
    "\n",
    "Next we apply two methods to extract general patterns and differences between clusters:\n",
    "1. Scatter plots for a number of interesting combinations of features. \n",
    "2. Histograms for each feature inside each cluster.\n",
    "\n",
    "#### Findings\n",
    "*The two clusters are hereby referred to as R (red) and B (blue).*\n",
    "\n",
    "- B tends to spend more than R in all the spend categories. \n",
    "- B has mostly no kids, while R usually has 1 kid. \n",
    "- Both B and R have the entire range of age. However, \n",
    "    - R has highest concentration around the age 35-55.\n",
    "    - B are generally older than R.\n",
    "- B prefers store purchases, whereas R prefers web.\n",
    "- R has fewer purchases (both web and store) than B. However, R has more web visits than B. \n",
    "- Customers with kids spend considerably less\n",
    "- Customers who accept four or five campaigns\n",
    "    - has higher education\n",
    "    - has fewer kids\n",
    "    - spend a lot on wine\n",
    "    - spend less on gold, sweets and fruits\n",
    "- R spends considerably less than R.\n",
    "- R has considerably less purchases than B.\n",
    "- B accepts more campaigns than R\n",
    "- Total spend is more affected by the average spend per purchase than by how many purchases the customer makes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unscale the data\n",
    "scaler = prep.get_scaler()\n",
    "unscaled = scaler.inverse_transform(data_2)\n",
    "unscaled_data_2 = pd.DataFrame(unscaled, index=data_2.index, columns=data_2.columns)\n",
    "\n",
    "# Extract cluster labels\n",
    "unscaled_data_2['Cluster'] = km_2.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(unscaled_data_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms\n",
    "plot_comparisons(unscaled_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three clusters\n",
    "\n",
    "### Differences between the two clusters\n",
    "\n",
    "In the parallel coordinates plot we see that the centers of R and G follow each other closely. If the clusters are dense (i.e. low variations within the clusters), it will be difficult to obtain good characteristics of the separate clusters.\n",
    " \n",
    "From the parallel coordinates plot and the standard deviations we see that in general the features can be categorized in features that are similar between the clusters (not defining), features that differs greatly (defining), and features that can be deemed neutral:\n",
    "\n",
    "**Defining**\n",
    "- `KidHome`\n",
    "- `Years_customer`\n",
    "- `MntWines`\n",
    "- `NumStorePurchases`\n",
    "- `MntFishMeatProdcts`\n",
    "\n",
    "**Neutral**\n",
    "- `MntFruits`\n",
    "- `MntSweetProducts`\n",
    "- `MntGoldProducts`\n",
    "- `NumWebPurchases`\n",
    "\n",
    "**Not defining**\n",
    "- `Education`\n",
    "- `Recency`\n",
    "- `NumWebVisitsMonth`\n",
    "- `Age`\n",
    "- `Num_AcceptedCmp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3, _ = prep.get_data(test_size=-1, remove_outliers=True, aggregate_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_3 = KMeans(n_clusters=3, random_state=0)\n",
    "km_3.fit(data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parallel(km_3, data_3, 3)\n",
    "plot_std(km_3, data_3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the clusters\n",
    "\n",
    "Next we apply two methods to extract general patterns and differences between clusters:\n",
    "1. Scatter plots for a number of interesting combinations of features. \n",
    "2. Histograms for each feature inside each cluster.\n",
    "\n",
    "#### Findings\n",
    "\n",
    "As implied by the parallel coordinates plot of the cluster centers, clusters 1 and 2 do not differ significantly enough to obtain any clear differences. They seem to follow roughly the same distribution, but cluster 2 has significantly less instances than cluster 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unscale the data\n",
    "scaler = prep.get_scaler()\n",
    "unscaled = scaler.inverse_transform(data_3)\n",
    "unscaled_data_3 = pd.DataFrame(unscaled, index=data_3.index, columns=data_3.columns)\n",
    "\n",
    "# Extract cluster labels\n",
    "unscaled_data_3['Cluster'] = km_3.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(unscaled_data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparisons(unscaled_data_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "To see whether visualization suggests a clustering in the data, we apply PCA with `p=2` and create a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data, _ = prep.get_data(-1, remove_outliers=False, aggregate_features=False)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2, random_state=0)\n",
    "principal_components = pca.fit_transform(data_2)\n",
    "\n",
    "print(f'Total explained vairance ratio for p=2: {np.sum(pca.explained_variance_ratio_)}')\n",
    "\n",
    "# Create a DataFrame for the PCA results\n",
    "pca_df = pd.DataFrame(data = principal_components, columns = ['Principal Component 1', 'Principal Component 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA with `p=2` captures approximately 45% the data set. Even though it not entirely accurate, the PCA visualization strongly suggests a clustering of 3 clusters (possibly 4 or 5) clusters. We therefore proceed by assigning the clusters from KMeans to see if they align with the suggested clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df_2 = pca_df.copy()\n",
    "pca_df_3 = pca_df.copy()\n",
    "\n",
    "pca_df_2['Cluster'] = km_2.labels_\n",
    "pca_df_3['Cluster'] = km_3.labels_\n",
    "\n",
    "# Plot the PCA results\n",
    "plt.figure(figsize=(8,6))\n",
    "scatter = plt.scatter(pca_df_2['Principal Component 1'], pca_df_2['Principal Component 2'], alpha=0.5, c=pca_df_2['Cluster'])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA')\n",
    "plt.colorbar(scatter)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the PCA results\n",
    "plt.figure(figsize=(8,6))\n",
    "scatter = plt.scatter(pca_df_3['Principal Component 1'], pca_df_3['Principal Component 2'], alpha=0.5, c=pca_df_3['Cluster'])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(scatter)\n",
    "plt.title('PCA')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters suggested by PCA does not have a sphrerical shape. If this reflects the true nature of the data, it makes sense that the cluster clustering obtained by Kmeans does not align with the suggested clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n",
    "To see whether visualization suggests a clustering in the data, we apply t-SNE with `n=2` and and `n=3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "data, _ = prep.get_data(-1, remove_outliers=True, aggregate_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne2 = TSNE(n_components=2, random_state=0)\n",
    "tsne_components = tsne2.fit_transform(data)\n",
    "tsne2_df = pd.DataFrame(data=tsne_components, columns=['tsne1', 'tsne2'])\n",
    "tsne2_df['Cluster'] = km_3.labels_\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='tsne1', y='tsne2', data=tsne2_df, s=100, alpha=0.6, c=tsne2_df['Cluster'])\n",
    "plt.title('t-SNE Visualization')\n",
    "plt.xlabel('tsne1')\n",
    "plt.ylabel('tsne2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "tsne3 = TSNE(n_components=3, random_state=0)\n",
    "tsne_components = tsne3.fit_transform(data)\n",
    "tsne3_df = pd.DataFrame(data=tsne_components, columns=['tsne1', 'tsne2', 'tsne3'])\n",
    "tsne3_df['Cluster'] = km_3.labels_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(tsne3_df, x='tsne1', y='tsne2', z='tsne3', color='Cluster',)\n",
    "\n",
    "\n",
    "fig.update_layout(title='3D t-SNE Plot ', \n",
    "                scene=dict(\n",
    "                xaxis_title='tsne1',\n",
    "                yaxis_title='tsne2',\n",
    "                zaxis_title='tsne3',       \n",
    "             ))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE with 2 and 3 dimensions suggests that there are 4-6 clusters in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other things we tried\n",
    "\n",
    "## DBSCAN\n",
    "\n",
    "Visualizing the data with PCA suggested that the clusters in the data does not have spherical shapes. As Kmeans performs poorly on non-spherical clusters, we apply DBSCAN to the data. PCA with three dimensions has an explained variance ratio of â‰ˆ 56,2%.\n",
    "\n",
    "Out of curiosity, we try to get the DBSCAN model to find the 3 clusters suggested by PCA. We obtain this by setting `eps=0.18` and `min_samples=12`.\n",
    "\n",
    "### Findings\n",
    "We "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "dbscan_df = pd.read_csv(\"customer_data_large.csv\")\n",
    "scaled = scaler.fit_transform(dbscan_df)\n",
    "dbscan_df = pd.DataFrame(scaled, index=dbscan_df.index, columns=dbscan_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=3, random_state=0)\n",
    "principal_components = pca.fit_transform(dbscan_df)\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2', 'Principal Component 3'])\n",
    "\n",
    "print(f'Explained variance ratio: {np.sum(pca.explained_variance_ratio_)}')\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.18, min_samples=12) \n",
    "clusters = dbscan.fit_predict(pca_df)\n",
    "\n",
    "sil = silhouette_score(pca_df, clusters)\n",
    "print(f'Silhouette score for clustering: {silhouette_score(pca_df, clusters)}')\n",
    "\n",
    "\n",
    "# Add clusters to the PCA df\n",
    "pca_df['Cluster'] = clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clusterings\n",
    "# 2 dimensions\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(pca_df['Principal Component 1'], pca_df['Principal Component 2'], c=pca_df['Cluster'], cmap='viridis', alpha=0.5)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('DBSCAN Clustering Results with PCA Reduction')\n",
    "plt.colorbar(scatter)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 3 dimensions\n",
    "fig = px.scatter_3d(pca_df, x='Principal Component 1', y='Principal Component 2', z='Principal Component 3',\n",
    "                color='Cluster',\n",
    "                color_continuous_scale=px.colors.qualitative.D3)  \n",
    "\n",
    "\n",
    "fig.update_layout(title='3D PCA Plot of Clusters',\n",
    "                scene=dict(\n",
    "                    xaxis_title='Principal Component 1',\n",
    "                    yaxis_title='Principal Component 2',\n",
    "                    zaxis_title='Principal Component 3'\n",
    "                ),\n",
    "                coloraxis_colorbar=dict(\n",
    "                    title='Cluster'\n",
    "                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dbscan_df.columns)\n",
    "# Add clusters to original data\n",
    "dbscan_df['Cluster'] = clusters\n",
    "\n",
    "# Filtering out noise points(-1 labels are considered noise in DBSCAN)\n",
    "clustered_data = dbscan_df[dbscan_df['Cluster'] != -1]\n",
    "\n",
    "# Calculating the mean of points within each cluster\n",
    "cluster_centers = clustered_data.groupby('Cluster').mean()\n",
    "\n",
    "# Creating a DataFrame from these mean centers\n",
    "centers_df = pd.DataFrame(cluster_centers, columns=dbscan_df.columns[:-1])  # exclude the Cluster column from means\n",
    "centers_df.reset_index(inplace=True)\n",
    "centers_df['Cluster'] = ['Cluster ' + str(x+1) for x in centers_df['Cluster']]  # Naming clusters for visualization\n",
    "\n",
    "# Plotting the parallel coordinates\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = parallel_coordinates(centers_df, 'Cluster', color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "plt.title('Approximate Centers of DBSCAN Clusters (Using Means)')\n",
    "plt.xticks(rotation=45)  # Rotating x-axis labels to be vertical\n",
    "plt.grid(True)\n",
    "plt.legend(title='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Compute the standard deviation for each variable across the cluster centers\n",
    "std_dev = cluster_centers.std()\n",
    "# Create a bar plot for the standard deviations\n",
    "plt.figure(figsize=(10, 6))\n",
    "std_dev.plot(kind='bar', color='skyblue')\n",
    "plt.title('Standard Deviation of Cluster Center Values Across Variables')\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Standard Deviation')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, linestyle='--', which='both', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plots we see that the only differentiating feature is `Years_customer`. We therefore try to remove it, and see if we can get some more insightful clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "dbscan_df = pd.read_csv(\"customer_data_large.csv\")\n",
    "dbscan_df = dbscan_df.drop('Years_customer', axis=1)\n",
    "scaled = scaler.fit_transform(dbscan_df)\n",
    "dbscan_df = pd.DataFrame(scaled, index=dbscan_df.index, columns=dbscan_df.columns)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=3, random_state=0)\n",
    "principal_components = pca.fit_transform(dbscan_df)\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2', 'Principal Component 3'])\n",
    "\n",
    "print(f'Explained variance ratio: {np.sum(pca.explained_variance_ratio_)}')\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.18, min_samples=12) \n",
    "clusters = dbscan.fit_predict(pca_df)\n",
    "\n",
    "sil = silhouette_score(pca_df, clusters)\n",
    "print(f'Silhouette score for clustering: {silhouette_score(pca_df, clusters)}')\n",
    "\n",
    "\n",
    "# Add clusters to the PCA df\n",
    "pca_df['Cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clusterings\n",
    "# 2 dimensions\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(pca_df['Principal Component 1'], pca_df['Principal Component 2'], c=pca_df['Cluster'], cmap='viridis', alpha=0.5)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('DBSCAN Clustering Results with PCA Reduction')\n",
    "plt.colorbar(scatter)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 3 dimensions\n",
    "fig = px.scatter_3d(pca_df, x='Principal Component 1', y='Principal Component 2', z='Principal Component 3',\n",
    "                color='Cluster',\n",
    "                color_continuous_scale=px.colors.qualitative.D3)  \n",
    "\n",
    "\n",
    "fig.update_layout(title='3D PCA Plot of Clusters',\n",
    "                scene=dict(\n",
    "                    xaxis_title='Principal Component 1',\n",
    "                    yaxis_title='Principal Component 2',\n",
    "                    zaxis_title='Principal Component 3'\n",
    "                ),\n",
    "                coloraxis_colorbar=dict(\n",
    "                    title='Cluster'\n",
    "                ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
