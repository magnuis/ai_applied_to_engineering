{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "There are no null-values or na values in the data. All values have dtype `int64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the heart dataset\n",
    "data = pd.read_csv(\"customer_data_large.csv\")\n",
    "\n",
    "# Check the dimensions of the dataset\n",
    "dataset_dimensions = data.shape\n",
    "\n",
    "print(\"The dataset has {} rows and {} colums.\".format(dataset_dimensions[0],dataset_dimensions[1]))\n",
    "\n",
    "print(f'The data has {data.isnull().sum().sum()} null values')\n",
    "print(f'The data has {data.isna().sum().sum()} na values')\n",
    "\n",
    "data.info()\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def show_numerical_figures(df):\n",
    "    columns = df.columns.tolist()\n",
    "    for col in columns:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.histplot(df[col],  bins=30)\n",
    "        plt.title(f'Histogram of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "show_numerical_figures(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate new features\n",
    "\n",
    "For this unsupervised problem, there are a number of features one can extract by combining exisitng features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TotalSpend'] = data[['MntWines', 'MntFruits', 'MntSweetProducts', 'MntGoldProds', 'NumWebVisitsMonth']].sum(axis=1)\n",
    "\n",
    "data['AvgSpendPerPurchase'] = data['TotalSpend'] / (data['NumWebPurchases'] + data['NumStorePurchases'] + 1)\n",
    "\n",
    "data['TotalPurchases'] = data[['NumWebPurchases', 'NumStorePurchases']].sum(axis=1)\n",
    "\n",
    "data['WebToStorePurchaseRatio'] = data['NumWebPurchases'] / (data['NumStorePurchases'] + 1)  # Adding 1 to avoid division by zero\n",
    "\n",
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check pairwise correlation\n",
    "\n",
    "# TODO - write about correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = data.corr(method='pearson')\n",
    "\n",
    "# Don't check pairwise relevances twices\n",
    "checked_features= []\n",
    "count = 0\n",
    "threshold = 0.7\n",
    "\n",
    "relevance_dict = {}\n",
    "\n",
    "for col in data.columns.tolist():\n",
    "    checked_features.append(col)\n",
    "    for val in matrix[col].items():\n",
    "        current_col = val[0]\n",
    "        relevance = val[1]\n",
    "        if relevance > threshold and current_col not in checked_features:\n",
    "            if relevance in relevance_dict.keys():\n",
    "                relevance_dict[relevance].append((col, current_col))\n",
    "            else:\n",
    "                relevance_dict[relevance] = [(col, current_col)]\n",
    "\n",
    "\n",
    "if not relevance_dict:\n",
    "    print(f'No variables with significance above {threshold}')\n",
    "else:\n",
    "    sorted_relevances = sorted(relevance_dict.keys(), reverse=True)\n",
    "    for relevance in sorted_relevances:\n",
    "        relationships = relevance_dict[relevance]\n",
    "        for relationship in relationships:\n",
    "            print(f'{relationship[0]} and {relationship[1]} have correlation {relevance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "To analyze outliers, we start by taking a look at box plots of features with potential outliers as well as identifying the number of instances falling out of range. These include the 'amount'-features, and the 'purchases'- and 'visits'-features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IQR_bounds(dataframe, column, lower_quantile=0.25, upper_quantile=0.75, should_print=True):\n",
    "    Q1, Q3 = dataframe[column].quantile([lower_quantile, upper_quantile])\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    low_outliers = (dataframe[column] < lower_bound).sum()\n",
    "    high_outliers = (dataframe[column] > upper_bound).sum()\n",
    "\n",
    "    if should_print:\n",
    "        print(f'{column} has {low_outliers} low outliers (below {lower_bound}) and {high_outliers} high outliers (above {upper_bound})')\n",
    "\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "possible_outlier_columns = ['MntWines', 'MntFruits',\n",
    "       'MntSweetProducts', 'MntGoldProds', 'NumWebPurchases',\n",
    "       'NumStorePurchases', 'NumWebVisitsMonth', 'MntFishMeatProdcts']\n",
    "# possible_outlier_columns = ['MntWines', 'MntFruits',\n",
    "#        'MntSweetProducts', 'MntGoldProds', 'NumWebPurchases',\n",
    "#        'NumStorePurchases', 'NumWebVisitsMonth', \n",
    "#        'Num_AcceptedCmp', 'MntFishMeatProdcts', 'TotalSpend',\n",
    "#        'AvgSpendPerPurchase', 'TotalPurchases', 'WebToStorePurchaseRatio']\n",
    "\n",
    "for feature in possible_outlier_columns:\n",
    "    IQR_bounds(data, feature, 0.25, 0.75)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.boxplot(x=data[feature])\n",
    "    plt.title(f'Boxplot of {feature}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we see that none of the features have any low outliers. We also see that\n",
    "- `MntWines`: Have 35 potential outliers. These may be important in a business perspective, as they can be a part of a customer segment (e.g. high-expenditure customers). As the majority of the instances are significantly lower than these high outliers, they may however skew standardization and make  the clustering poorer. We will try to both remove and keep them, and see what makes the clustering better.  \n",
    "\n",
    "- `MntFruits`: All possible outliers are evenly spread from Q3 to max. The max is however more than twice the Q3 limit, and including them will significanly skew the standardization. We will try to both include all, and to remove all instances above 2\\*Q3. By removing the instances above 2\\*Q3, we will still keep some of these potentially important values and not skew the standardization too much.\n",
    "\n",
    "-  `MntSweetProducts`: The majority of possible outliers increase gradually from Q3, whereas two customers have spent significantly more (more than 25%). The gradual increase from Q3 suggests that these may be an important customer segment, whereas the two lone outliers will be removed. \n",
    "\n",
    "- `MntGoldProds`: As with `MntSweetProducts`, the outliers increase gradually and intensively from Q3 to 250 while four are significantly higher. These four will be removed. \n",
    "\n",
    "- `NumWebPurchases`: Only 11 outliers. Most of them are close to the Q3, whereas four are significantly higher. We will try to both remove all (as there are few outliers) and to only remove the highest four. \n",
    "\n",
    "- `NumStorePurchases`: No outliers. \n",
    "\n",
    "- `NumWebVisitsMonth`: 148 potential high outliers. The values do however not vary greatly, and these high web visits may be of importance. due to the possible importance and the significant amount, they will all be kept. \n",
    "\n",
    "- `MntFishMeatProducts`: The majority of possible outliers increase gradually from Q3, whereas a few customers have spent significantly more. The significant amount of outliers close to Q3, and their potential value, forces ut to keep them, while the highest outliers will be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    _, bound_fruit = IQR_bounds(df, 'MntFruits', should_print=False)\n",
    "    _, bound_web = IQR_bounds(df, 'NumWebPurchases', should_print=False)\n",
    "\n",
    "    outliers_wine = df[(df['MntWines'] > 1400)].index\n",
    "    print(f'Removing {len(outliers_wine)} wine outliers')\n",
    "    df = df.drop(outliers_wine)\n",
    "    \n",
    "    outliers_fruit = df[(df['MntFruits'] > bound_fruit * 2)].index\n",
    "    print(f'Removing {len(outliers_fruit)} fruit outliers')\n",
    "    df = df.drop(outliers_fruit)\n",
    "    \n",
    "    outliers_sweets = df[(df['MntSweetProducts'] > 200)].index\n",
    "    print(f'Removing {len(outliers_sweets)} sweets outliers')\n",
    "    df = df.drop(outliers_sweets)\n",
    "\n",
    "    outliers_gold = df[(df['MntGoldProds'] > 250)].index\n",
    "    print(f'Removing {len(outliers_gold)} gold outliers')\n",
    "    df = df.drop(outliers_gold)\n",
    "\n",
    "    outliers_web_1 = df[(df['NumWebPurchases'] > bound_web)].index\n",
    "    outliers_web_2 = df[(df['NumWebPurchases'] > 20)].index\n",
    "    # print(f'Removing {len(outliers_web_1)} web purchase outliers')\n",
    "    # df = df.drop(outliers_web_1)\n",
    "    print(f'Removing {len(outliers_web_2)} web purchase outliers')\n",
    "    df = df.drop(outliers_web_2)\n",
    "\n",
    "    outliers_fish_meat = df[(df['MntFishMeatProdcts'] > 1250)].index\n",
    "    print(f'Removing {len(outliers_fish_meat)} fish_meat outliers')\n",
    "    df = df.drop(outliers_fish_meat)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_outliers(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data in testing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale data\n",
    "In the dataset the scale of the features varies greatly; from 0-3 in `Education` and 0-1727 in `MntFishMeatProdcts`. In order to use k-means without bias towards the higher-scale features, we apply standardizaton through `sklearn.StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "test_scaled = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "*In order to explore the result of the different preprocessing techniques effectively, we have created a `Preprocessor` class and extracted the above methods to it. For the remaining part of the notebook, we utilize this class for preprocessing.*\n",
    "\n",
    "### Clustering evaluation metrics\n",
    "**Silhouette score:** The cohesion and separation of clusters. Ranges from -1 to 1, with higher values indicating better clustering.\n",
    "\n",
    "**Davies-Bouldin score:** The ratio of within-cluster distances to between-cluster distances. A lower value is better.\n",
    "\n",
    "**Inertia:** The sum of squared distances between samples and their cluster means. A lower value indicates better clustering. \n",
    "\n",
    "**Distortion:** The average of squared distances between samples and their cluster means. A lower value indicates better clustering.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from preprocessor import Preprocessor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "prep = Preprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying K-Means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(df):\n",
    "    '''Plot change of inertia for different values of k, \n",
    "    to get a reasonable guess for optimal amount of clusters'''\n",
    "    db_scores = []\n",
    "    db_scores_test = []\n",
    "    silhouettes = []\n",
    "    silhouettes_test = []\n",
    "    distortions = []\n",
    "    inertias = []\n",
    "    K = range(1, 15)\n",
    "    \n",
    "    for k in K:\n",
    "        distortion = []\n",
    "        inertia = []\n",
    "        silhouette = []\n",
    "        silhouette_test = []\n",
    "        db = []\n",
    "        db_test = []\n",
    "\n",
    "        # Apply K-fold \n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        for train_index, test_index in kf.split(df):\n",
    "        \n",
    "            # Prepare DataFrames\n",
    "            train = df.iloc[train_index]\n",
    "            train = pd.DataFrame(data=train, columns=df.columns)\n",
    "\n",
    "            test = df.iloc[test_index]\n",
    "            test = pd.DataFrame(data=test, columns=df.columns)\n",
    "\n",
    "            # Fit the model and make predictions\n",
    "            kmeanModel = KMeans(n_clusters=k, random_state=42).fit(train)\n",
    "            labels = kmeanModel.predict(train)\n",
    "            test_labels = kmeanModel.predict(test)\n",
    "\n",
    "            # Calculate evaluation metrics\n",
    "            distortion.append(sum(np.min(cdist(df, kmeanModel.cluster_centers_,\n",
    "                                                'euclidean'), axis=1)) / df.shape[0])\n",
    "            inertia.append(kmeanModel.inertia_)\n",
    "    \n",
    "            # Can only use silhouette_score and davies_bouldin_score if there are more than one cluster\n",
    "            if len(np.unique(labels)) > 1: \n",
    "                silhouette.append(silhouette_score(train, labels))\n",
    "                db.append(davies_bouldin_score(train, labels))\n",
    "\n",
    "            # Can only use silhouette_score and davies_bouldin_score if there are more than one cluster\n",
    "            if len(np.unique(test_labels)) > 1: \n",
    "                silhouette_test.append(silhouette_score(test, test_labels))\n",
    "                db_test.append(davies_bouldin_score(test, test_labels))\n",
    "\n",
    "        # Append mean of all folds\n",
    "        distortions.append(np.mean(distortion))\n",
    "        inertias.append(np.mean(inertia))\n",
    "        silhouettes_test.append(np.mean(silhouette_test))\n",
    "        db_scores_test.append(np.mean(db_test))\n",
    "        silhouettes.append(np.mean(silhouette))\n",
    "        db_scores.append(np.mean(db))\n",
    "\n",
    "\n",
    "    # Plot distortion\n",
    "    plt.plot(K, distortions, 'bx-')\n",
    "    plt.xlabel('Values of K')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('The Elbow Method using Distortion')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot inertia\n",
    "    plt.plot(K, inertias, 'bx-')\n",
    "    plt.xlabel('Values of K')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('The Elbow Method using Inertia')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot silhouette score, both on test and train data\n",
    "    plt.figure(figsize=(14, 5))  \n",
    "    plt.subplot(1,2,1)\n",
    "    sns.lineplot(x=K, y=silhouettes, marker='o', sort=False, color='blue')\n",
    "    sns.lineplot(x=K, y=silhouettes_test, marker='o', sort=False, color='red')\n",
    "    plt.xlabel('Values of K')\n",
    "    plt.ylabel('Silhouette score')\n",
    "    plt.title('The Elbow Method using Silhouette score')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot db score, both on test and train data\n",
    "    plt.figure(figsize=(14, 5))  \n",
    "    plt.subplot(1,2,1)\n",
    "    sns.lineplot(x=K, y=db_scores, marker='o', sort=False, color='blue')\n",
    "    sns.lineplot(x=K, y=db_scores_test, marker='o', sort=False, color='red')\n",
    "    plt.xlabel('Values of K')\n",
    "    plt.ylabel('DB score')\n",
    "    plt.title('The Elbow Method using DB score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_data, _ = prep.get_data(test_size=-1)\n",
    "\n",
    "\n",
    "elbow_method(elbow_data)\n",
    "elbow_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow method using inertia, silhouette score and db score all suggests the optimal value of k is for `k=6`. The elbow method using distortia suggest the optimal value is `k=5`. \n",
    "\n",
    "None of the plots suggests strong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characerize the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_clusters = 5  # Assuming 5 is optimal from the elbow method\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(train_scaled)\n",
    "\n",
    "train = pd.DataFrame(scaler.inverse_transform(train_scaled), index=train.index, columns=train.columns)\n",
    "\n",
    "centroids = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster centroids\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to your original DataFrame\n",
    "train['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Calculate mean for all numerical columns for each cluster\n",
    "cluster_profiles = train.groupby('Cluster').mean()\n",
    "\n",
    "# Display cluster profiles\n",
    "print(cluster_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example: Visualizing the distribution of a feature like 'TotalSpend' across clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Cluster', y='TotalSpend', data=train)\n",
    "plt.title('Total Spend by Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "all_data_scaler = StandardScaler()\n",
    "data_scaled = all_data_scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One principal component\n",
    "\n",
    "PCA with `n_components=1` gives an explained variance ratio of ≈0.364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_1 = PCA(n_components=1)\n",
    "pca_1_components = pca_1.fit_transform(data_scaled)\n",
    "\n",
    "print(pca_1.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_1_df = pd.DataFrame(data=pca_1_components, columns=['pca1'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_1_df.index, pca_1_df['pca1'], alpha=0.6)\n",
    "plt.title('PCA: One Principal Component')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('pca1')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(pca_1_df['pca1'], bins=30, kde=True)\n",
    "plt.title('Distribution of pca1')\n",
    "plt.xlabel('pca1')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two principal components\n",
    "\n",
    "PCA with `n_components=2` gives an explained variance ratio of ≈0.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_2_components = pca_2.fit_transform(data_scaled)\n",
    "\n",
    "print(pca_2.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2_df = pd.DataFrame(data=pca_2_components, columns=['pca1', 'pca2'])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='pca1', y='pca2',  data=pca_2_df, palette='viridis', s=100, alpha=0.6)\n",
    "plt.title('PCA: Two Principal Components')\n",
    "plt.xlabel('pca1')\n",
    "plt.ylabel('pca2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA with neither one nor two principal components serves to identify possible clusters in the dataset. \n",
    "\n",
    "We further try to apply PCA with a higher number of `n_components` and train a k-means model using the resulting principal components. We then compare the clustering we get from this model to the clusterings from kmeans on the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
