{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Check the dimensions of the dataset (number of rows and columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the heart dataset\n",
    "heart_data = pd.read_csv(\"heart.csv\")\n",
    "\n",
    "# Check the dimensions of the dataset\n",
    "dataset_dimensions = heart_data.shape\n",
    "\n",
    "print(\"The dataset has {} rows and {} colums.\".format(dataset_dimensions[0],dataset_dimensions[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Peek at the first few rows to understand the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Adjust pandas display settings\n",
    "pd.set_option('display.max_columns', 12)  # Ensure all columns are attempted to be displayed\n",
    "pd.set_option('display.width', 1000)  # Adjust the width \n",
    "\n",
    "\n",
    "# Display the first few rows of heart.csv\n",
    "print(heart_data.head())\n",
    "print(heart_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Examine the types of data present in each column (numerical, categorical, datetime,...). Verify that the data types assigned to each column align with the actual nature of the data. Convert data types if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to display data types \n",
    "def display_dtypes(dataframe):\n",
    "    # Create a DataFrame from the dtypes\n",
    "    dtypes_df = pd.DataFrame(dataframe.dtypes, columns=['Data Type'])\n",
    "    # Reset index to get the column names as a separate column\n",
    "    dtypes_df.reset_index(inplace=True)\n",
    "    # Rename columns \n",
    "    dtypes_df.columns = ['Column Name', 'Data Type']\n",
    "    # Display the DataFrame\n",
    "    display(dtypes_df)  \n",
    "\n",
    "# Display original data types\n",
    "print(\"Original Data Types:\")\n",
    "display_dtypes(heart_data)\n",
    "\n",
    "# Convert categorical variables to 'category' data type\n",
    "categorical_columns = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "heart_data[categorical_columns] = heart_data[categorical_columns].astype('category')\n",
    "\n",
    "# Display data types after conversion for comparison\n",
    "print(\"\\nData Types after Conversion:\")\n",
    "display_dtypes(heart_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Filter the data by removing variables that are not relevant for the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the context of predicting heart failure, all of the 12 variables have relevant associations with cardiovascular health and could potentially contribute valuable insights when building a predictive model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Univariate analysis: Summary statistics for numerical variables and frequency distribution for categorical ones. Create visualizations (histograms, box plots, bar plots, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Set the style of the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Summary statistics for numerical variables\n",
    "print(\"Summary Statistics for Numerical Variables:\")\n",
    "print(heart_data.describe())\n",
    "\n",
    "# Histograms for numerical variables\n",
    "numerical_columns = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
    "for col in numerical_columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(heart_data[col], kde=True, bins=30)\n",
    "    plt.title(f'Histogram of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "# Frequency distribution and bar plots for categorical variables\n",
    "categorical_columns = ['Sex', 'ChestPainType', 'FastingBS', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "for col in categorical_columns:\n",
    "    print(f\"Frequency distribution for {col}:\")\n",
    "    print(heart_data[col].value_counts())\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(x=col, data=heart_data)\n",
    "    plt.title(f'Bar Plot of {col}')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Identify outliers and decide whether to remove, transform, or keep them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RestingBP: \n",
    "\n",
    "- Normal blood pressure for adults ranges from 90/60 mm KG to 120/80 mm Hg. High blood pressure is a critical risk factor for heart disease, these high values are therefore valuable for our prediction and we have kept them in our analysis. \n",
    "- We have decided to remove the 0 values. These values do not contribute to a meaningful analysis, because RestingBP=0 is not poosible. \n",
    "\n",
    "Cholesterol: \n",
    "- High cholesterol levels are clinically relevant and can indicate a significant risk for cardiovascular diseases. \n",
    "- The instances where cholesterol is recorded as 0 are clearly errors or missing data, as it's physiologically impausible for someone to have no cholesterol. \n",
    "\n",
    "MaxHR: \n",
    "- MaxHR can vary widely among individuals, especially considering factors like age, fitness level, and the presence of cardiovascular conditions. Given this variability and its clinical significance, we have decided to keep the two low outliers in MaxHR. \n",
    "\n",
    "\n",
    "Oldpeak: \n",
    "- Oldpeak\" refers to the ST depression induced by exercise relative to rest, measured in millimeters (mm) on an electrocardiogram (ECG). ST depression is a finding on an ECG that can indicate myocardial ischemia, a condition where part of the heart does not receive enough oxygen, often due to blockages in the coronary arteries. In the context of a stress test, an increase in oldpeak value (more negative or more pronounced depression) can suggest a higher likelihood of coronary artery disease.\n",
    "- ST Elevation: An ST amplitude of ≥0.1 mV, ≥0.15 mV, and ≥0.2 mV is considered significant for ST elevation. ST elevation can indicate acute myocardial infarction (heart attack) and other conditions that lead to an elevated risk of heart disease. High positive Oldpeak values (e.g., 4.0, 5.6, 6.2) are well above the ≥0.2 mV elevation significance threshold, indicating substantial ST segment deviations. These are critical for identifying potential heart disease risk and should be retained for their clinical significance.\n",
    "- ST Depression: For ST depression, thresholds of ≤–0.05 mV or ≤–0.1 mV are used to denote clinical significance. ST depression is indicative of myocardial ischemia, a condition where the heart muscle doesn't receive enough oxygen, often due to narrowed or blocked coronary arteries. With ST depression being clinically significant at values of ≤–0.05 mV or ≤–0.1 mV, a negative Oldpeak value reflects ST depression and is thus clinically relevant. This value suggests myocardial ischemia and should be included in analyses concerning heart disease risk, assuming the negative sign indicates depression below the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numerical variables to check for outliers\n",
    "numerical_variables = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
    "\n",
    "# Function that calculates the outliers based on the IQR method\n",
    "# The IQR is the difference between the 75th percentile (Q3) and the 25th percentile (Q1) of the data\n",
    "# Outliers are typically considered data points that fall below (Q1- 1,5*IQR) or above (Q3 + 1,5*IQR)\n",
    "\n",
    "def IQR_method(dataframe, colum):\n",
    "    Q1 = dataframe[colum].quantile(0.25)\n",
    "    Q3 = dataframe[colum].quantile(0.75)\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    return dataframe[(dataframe[colum] < lower_bound) | (dataframe[colum] > upper_bound)]\n",
    "\n",
    "# Identify outliers in each numerical variable (Age, RestingBP, Cholesterol, FastingBS, MaxHR and Oldpeak)\n",
    "\n",
    "def display_outliers(dataframe, variables):\n",
    "    for variable in variables:\n",
    "        outliers_df = IQR_method(dataframe, variable)\n",
    "        print(f\"Number of outliers in {variable}: {len(outliers_df)}\")\n",
    "        if len(outliers_df) > 0:\n",
    "            display(outliers_df[[variable]])\n",
    "\n",
    "print(\"Outliers before removing 0 values:\")\n",
    "display_outliers(heart_data, numerical_variables)\n",
    "\n",
    "# Remove 0 values from RestingBP and Cholesterol\n",
    "heart_data_cleaned = heart_data[(heart_data['RestingBP'] > 0) & (heart_data['Cholesterol'] > 0)]\n",
    "\n",
    "print(\"\\nOutliers after removing 0 values for RestingBP and Cholesterol:\")\n",
    "display_outliers(heart_data_cleaned, numerical_variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Identify variables that provide little to no information and remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skal vi fjerne dette punktet siden vi gjorde det i punkt 4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Bivariate analysis: Examine correlations and dependencies between variables to identify potential relationships. Create visualizations to explore these relationships. A particularly significant case involves assessing the relationship of each variable with the class/target variable you want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: sammenligning med ikke-numeriske verdier\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define variables of interest, they will be compared to the target variable HeartDisease\n",
    "variables = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
    "\n",
    "# Calculate correlations and print them\n",
    "for variable in variables:\n",
    "    correlation = heart_data[[variable, 'HeartDisease']].corr()\n",
    "    print(correlation)\n",
    "\n",
    "# Calculate correlations\n",
    "correlation_data = []\n",
    "for variable in variables:\n",
    "    correlation = heart_data[[variable, 'HeartDisease']].corr().iloc[0, 1]\n",
    "    correlation_data.append(correlation)\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "correlation_df = pd.DataFrame({\n",
    "    'Variable': variables,\n",
    "    'Correlation': correlation_data\n",
    "})\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(correlation_df['Variable'], correlation_df['Correlation'], color='skyblue')\n",
    "plt.title('Correlation with Heart Disease')\n",
    "plt.xlabel('Variable')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.ylim(-1, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Create new features from existing ones if necessary (e.g., extracting date components, ratios between variables, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an age group column from 20-40, 40-60 and 60+\n",
    "binned = pd.cut(heart_data['Age'], bins=[20, 40, 60, 100], labels=['Young Adult', 'Middle-aged', 'Elderly'])\n",
    "\n",
    "# Add the binned values as a new categorical feature\n",
    "heart_data[\"Age_group\"] = binned\n",
    "print(heart_data[['Age', 'Age_group']].head())\n",
    "heart_data.groupby('Age_group', observed=False).size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Divide the dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You train the model using the training set. You test the model using the testing set.\n",
    "\n",
    "# Shuffle dataframe using sample function\n",
    "shuffled_data = heart_data.sample(frac=1)\n",
    "shuffled_data.head()\n",
    "\n",
    "# Select ratio - how much of the data will be used for training and how much for testing\n",
    "ratio = 0.75\n",
    "\n",
    "# total number of rows in the shuffled dataset\n",
    "total_rows = shuffled_data.shape[0]\n",
    "# the number of rows to include in the training set\n",
    "train_size = int(total_rows*ratio)\n",
    " \n",
    "# Split data into test and train\n",
    "train = shuffled_data[0:train_size]\n",
    "test = shuffled_data[train_size:]\n",
    "\n",
    "# print train set\n",
    "print(\"Train dataframe\")\n",
    "print(train)\n",
    " \n",
    "# print test set\n",
    "print(\"Test dataframe\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Scale numerical features to a similar range in order to improve model performance on some models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the desired values\n",
    "df = pd.DataFrame(heart_data[['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']])\n",
    "\n",
    "# Min-max scaling\n",
    "df_scaled = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "print(df_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Encode categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical columns\n",
    "categorical_columns = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "\n",
    "# Create new columns with encoded values\n",
    "for column in categorical_columns:\n",
    "    new_column_name = f\"{column}_encoded\"\n",
    "    heart_data[new_column_name] = heart_data[column].cat.codes\n",
    "\n",
    "# Print DataFrame to verify the new columns\n",
    "print(heart_data[['Sex_encoded', 'ChestPainType_encoded', 'RestingECG_encoded', 'ExerciseAngina_encoded', 'ST_Slope_encoded']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Identify missing data and decide how to handle them (remove, impute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing data\n",
    "\n",
    "# Column names with the number of non-null values in each column\n",
    "# num_missing = heart_data.info()\n",
    "# print(num_missing)\n",
    "\n",
    "# summarize total number of missing values per column\n",
    "num_missing1 = heart_data.isnull().sum()\n",
    "print(num_missing1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
