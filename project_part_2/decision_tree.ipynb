{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO - Strange values\n",
    "\n",
    "# Decision Tree - Heart Disease Classification\n",
    "Part 2 of the course project in *Artificial intelligence applied to engineering* at ETSEIB, UPC, spring 2024. The team members contributing to the deliverable is \n",
    "- Lise Jakobsen\n",
    "- Julie SÃ¸rlie Lund\n",
    "- Magnus Ingnes Sagmo\n",
    "\n",
    "The dataset used in this deliverable can be retrieved from [Kaggle](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset).\n",
    "\n",
    "## Data set review\n",
    "\n",
    "#### Target class\n",
    "The target class, `HeartDisease`, is a boolean class telling whether the patient has a heart disease. Subsequently, this is a classification problem.\n",
    "\n",
    "#### Features\n",
    "The data set has 11 features. Two of these, `ChestPainType` and `RestingECG`, are one-hot encoded. After the one-hot encoding we have 16 features.\n",
    "\n",
    "#### Preprocessing method\n",
    "Before applying KNN\n",
    "- zero values of `Cholesterol` are imputed using `sklearn.impute.KNNImputer`.\n",
    "- the data is split into train and test set using `sklearn.model_selection.train_test_split`.\n",
    "- the data is normalized using `sklearn.preprocessing.StandardScaler`.\n",
    "\n",
    "In addition, we will try to both keep and remove outliers to see what produces best predictions. \n",
    "\n",
    "## Performance metrics\n",
    "\n",
    "#### Recall\n",
    "In the case of detecting heart diseases it is crucial to minimize the number of false negatives (people with a heart disease going unnoticed). We will therefore focus on minimizing recall.\n",
    "\n",
    "#### F1-score\n",
    "By solely focusing on recall, we can end with a too high number of false positives (by choosing a model that classifies everything as heart disease). We will therefore also look at the F1-score, as it offers a balance between precision and recall. \n",
    "\n",
    "#### Precision-recall curve\n",
    "The precision-recall curve is a good way to visualize how well the model balances precision and recall. \n",
    "\n",
    "#### Confusion matrix\n",
    "A good way to look at the number of false positives is looking at the confusion matrix. This will allow us to simultaneously look at false negatives, true positives and true negatives. \n",
    "\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "#### Max Depth\n",
    "The `max_depth` hyperparameter decides the maximum depth of the tree. Low max depth makes the model generalize, but may lead to underfitting. Conversely, a high max depth enables the model to learn many details, but may perform poorly on unseen data (overfitting). \n",
    "\n",
    "#### Minimum Split Samples\n",
    "`min_samples_split`: The minimum number of samples required to split a node. Higher values prevent the model from learning too specific patterns, thus controlling overfitting.\n",
    "\n",
    "#### Minimum Leaf Samples\n",
    "The `min_samples_leaf` minimum number of samples a node needs to be considered a leaf. Helps avoid overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "We will split the data in training and test set, and only use the train data for the cross validation in the grid search. This will reduce the amount of data used to train the models and tuning hyperparameters, butt will ensure no data leakage and help reduce overfitting of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from preprocessor import Preprocessor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score, f1_score, precision_recall_curve, auc, precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = Preprocessor()\n",
    "\n",
    "X_train, X_test, y_train, y_test = prep.get_data(test_size=0.25, impute_method='knn', remove_outliers=True, scaling_method=None)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up hyperparameters grid\n",
    "\n",
    "We will explore\n",
    "- `max_depth`: Numbers 2-16 (there are 16 features).\n",
    "- `min_samples_split`: [2, 5, 10, 15, 20].\n",
    "- `min_samples_leaf`: [1, 3, 5, 10, 15]. (We initially included 10 and 15 as well, but none of the 25 best results had ``min_samples_leaf` above 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': np.arange(2, 17),  \n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 3, 5, 10, 15]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform grid search\n",
    "\n",
    "As stated above, we will focus mainly on recall and F1-score for our model evaluation. For the grid search we will use F1-score as evaluation metric, because\n",
    "- using recall can cause an unacceptably poor precision.\n",
    "- F1-score also considers true positives.\n",
    "\n",
    "To narrow the number of combinations we will further evaluate, we retrieve the ten best combinations from the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "rf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='f1', return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract the ten best iterations, based on mean F1 score\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "top_10 = results.nlargest(10, 'mean_test_score')  \n",
    "\n",
    "# Extract only the interesting columns from the DataFrame\n",
    "top_10 = top_10[['param_max_depth', 'param_min_samples_split', 'param_min_samples_leaf', 'mean_test_score', 'std_test_score']]\n",
    "\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50 = results.nlargest(50, 'mean_test_score')  \n",
    "# Extract only the interesting columns from the DataFrame\n",
    "top_50 = top_50[['param_max_depth', 'param_min_samples_split', 'param_min_samples_leaf', 'mean_test_score', 'std_test_score']]\n",
    "\n",
    "top_50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ten best combinations using F1-score are\n",
    "- in the range 7-16 for `max_depth`.\n",
    "- in the range 2-10 for `min_samples_split`. The majority (6/10) are 5.\n",
    "- in the range 1-3 for `min_samples_leaf`, with 9/10 being 1.\n",
    "\n",
    "\n",
    "We therefore do another round of grid search where we\n",
    "- remove the `min_samples_split=20` and `min_samples_split=15` options.\n",
    "- have a higher granularity for low `min_samples_split` values and low `min_samples_leaf` values.\n",
    "- only use the range of `max_depth` inside the top 10 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the updated parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': np.arange(3, 8),  \n",
    "    'min_samples_split': [2, 3, 4, 5, 7, 10, 11],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define model\n",
    "rf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='f1', return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract the five best iterations, based on mean F1 score\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "top_10 = results.nlargest(10, 'mean_test_score')  \n",
    "# Extract only the interesting columns from the DataFrame\n",
    "top_10 = top_10[['param_max_depth', 'param_min_samples_split', 'param_min_samples_leaf', 'mean_test_score', 'std_test_score']]\n",
    "\n",
    "top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating top 10 combinations\n",
    "We will continue by examining how the top ten results from round two perform on unseen data. We will look at \n",
    "- F1-score\n",
    "- Recall\n",
    "- Precision\n",
    "- Precision-Recall Area Under Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Make prediction\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Retrieve scores\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    pr_auc = auc(recall_curve, precision_curve)\n",
    "\n",
    "    return recall, f1, precision, pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame with evaluation metric for each of the top 10 models\n",
    "evaluation_results = pd.DataFrame(columns=['max_depth', 'min_samples_split', 'min_samples_leaf', 'Recall', 'F1-score', 'Precision', 'Precision-recall AUC'])\n",
    "\n",
    "# Iterate the top 10 parameter combinations\n",
    "for (index, row) in top_10.iterrows():\n",
    "\n",
    "    # Dict with params from the row\n",
    "    params = {\n",
    "        'max_depth': row['param_max_depth'],\n",
    "        'min_samples_split': row['param_min_samples_split'],\n",
    "        'min_samples_leaf': row['param_min_samples_leaf'],\n",
    "    }\n",
    "\n",
    "    # Fit the model with params\n",
    "    model = DecisionTreeClassifier(max_depth=params['max_depth'], min_samples_split=params['min_samples_split'], min_samples_leaf=params['min_samples_leaf'], random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    recall, f1, precision, pr_auc = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    new_row = {\n",
    "        'max_depth': params['max_depth'],\n",
    "        'min_samples_split': params['min_samples_split'],\n",
    "        'min_samples_leaf': params['min_samples_leaf'],\n",
    "        'Recall': round(recall, 3), \n",
    "        'Precision': round(precision, 3), \n",
    "        'F1-score': round(f1, 3), \n",
    "        'Precision-recall AUC': round(pr_auc, 3)\n",
    "    }\n",
    "    evaluation_results.loc[len(evaluation_results)] = new_row\n",
    "\n",
    "# Sort the values\n",
    "evaluation_results = evaluation_results.sort_values(by=['Recall', 'F1-score'], axis=0, ascending=False)\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results on the unseen data, we will further evaluate a model using `max_depth=10`, `min_samples_split=10` and `min_samples_leaf=1`. This is due to the following considerations:\n",
    "- The Recall rate is best. \n",
    "- The F1-score is best. \n",
    "- The Precision rate is best. \n",
    "- The Precision-recall AUC is best.\n",
    "- Having a lower `max_depth` value causes the model to generalize better, at the cost of possibly underfitting. When the best values of all score have relatively low `max_depth` value (10), we opt for this combination.\n",
    "- Similarly to `max_depth`, high `min_samples_split` values avoids overfitting at the cost of possibly performing worse. Because the top value have `min_samples_split=10`, we opt for this combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and variance\n",
    "\n",
    "Next, we examine the model performance in the context of bias and variance by plotting a validation curve for n in the range 1 to 29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validation_curve(param_name, param_range):\n",
    "    # Calculate scores for validation curve\n",
    "    train_scores, test_scores = validation_curve(\n",
    "        DecisionTreeClassifier(random_state=42),\n",
    "        X_train, y_train, param_name=param_name, \n",
    "        param_range=param_range,\n",
    "        cv=5, scoring=\"recall\", n_jobs=-1)\n",
    "\n",
    "    # Calculate mean and standard deviation for train and test scores\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot the validation curve\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(param_range, train_mean, label=\"Training score\", color=\"r\")\n",
    "    plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"g\")\n",
    "\n",
    "    plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, color=\"r\", alpha=0.1)\n",
    "    plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, color=\"g\", alpha=0.1)\n",
    "\n",
    "    plt.title(\"Validation Curve for Random Forest, recall\")\n",
    "    plt.xlabel(f\"{param_name}\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_range = np.arange(1, 15)\n",
    "plot_validation_curve('max_depth', md_range)\n",
    "\n",
    "split_range = np.arange(1, 20)\n",
    "plot_validation_curve('min_samples_split', split_range)\n",
    "\n",
    "leaf_range = np.arange(1, 20)\n",
    "plot_validation_curve('min_samples_leaf', leaf_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Depth\n",
    "The training score starts low, but approaches 100% as `max_depth` exceeds 8. The cross validation scores remain relatively stable for all values, with a small peak at `max_depth=3`. This may suggest that our guess of `max_depth=10` might bee too high.\n",
    "\n",
    "#### Minimum Split Samples\n",
    "The training scores are, as expected, decreasing as `min_samples_split` increases, with a slight drop in decrease rate at `min_samples_split=11`. The cross validation scores are relatively stable, with a small local peak at `min_samples_split=11`. Our guess of `min_samples_split=10` might be a good choice.\n",
    "\n",
    "### Minimum Leaf Samples\n",
    "The training scores are decreasing rapidly until around `min_samples_leaf=6`, before stabilizing. The cross validation scores seems to be relatively stable. Thus the choice of `min_samples_leaf=1` does not appear to have a negative impact on the performance of the model. \n",
    "\n",
    "### Conclusion\n",
    "Based on the validation curves, max_depth=10 may be too high as the best performance is seen at a lower depth, suggesting a shallower tree could effectively prevent overfitting. The setting of min_samples_split=10 is justified by the stable cross-validation scores, indicating it's a reasonable choice. The choice of min_samples_leaf=1, despite the rapid decline in training scores, does not adversely affect cross-validation scores, supporting its continued use. Overall, while adjustments could be explored, particularly in reducing max_depth, the initial settings appear largely appropriate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
