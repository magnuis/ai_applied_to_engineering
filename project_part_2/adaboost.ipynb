{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN - Heart Disease Classification\n",
    "Part 2 of the course project in *Artificial intelligence applied to engineering* at ETSEIB, UPC, spring 2024. The team members contributing to the deliverable is \n",
    "- Lise Jakobsen\n",
    "- Julie SÃ¸rlie Lund\n",
    "- Magnus Ingnes Sagmo\n",
    "\n",
    "The dataset used in this deliverable can be retrieved from [Kaggle](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset).\n",
    "\n",
    "## Data set review\n",
    "\n",
    "#### Target class\n",
    "The target class, `HeartDisease`, is a boolean class telling whether the patient has a heart disease. Subsequently, this is a classification problem.\n",
    "\n",
    "#### Features\n",
    "The data set has 11 features. Two of these, `ChestPainType` and `RestingECG`, are one-hot encoded. \n",
    "\n",
    "#### Preprocessing method\n",
    "Before applying KNN\n",
    "- zero values of `Cholesterol` are imputed using `sklearn.impute.KNNImputer`.\n",
    "- the data is split into train and test set using `sklearn.model_selection.train_test_split`.\n",
    "- the data is normalized using `sklearn.preprocessing.StandardScaler`.\n",
    "\n",
    "In addition, we will try to both keep and remove outliers to see what produces best predictions. \n",
    "\n",
    "## Performance metrics\n",
    "\n",
    "#### Recall\n",
    "In the case of detecting heart diseases it is crucial to minimize the number of false negatives (people with a heart disease going unnoticed). We will therefore focus on minimizing recall.\n",
    "\n",
    "#### F1-score\n",
    "By solely focusing on recall, we can end with a too high number of false positives (by choosing a model that classifies everything as heart disease). We will therefore also look at the F1-score, as it offers a balance between precision and recall. \n",
    "\n",
    "#### Precision-recall curve\n",
    "The precision-recall curve is a good way to visualize how well the model balances precision and recall. \n",
    "\n",
    "#### Confusion matrix\n",
    "A good way to look at the number of false positives is looking at the confusion matrix. This will allow us to simultaneously look at false negatives, true positives and true negatives. \n",
    "\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "#### Number of neighbors\n",
    "The most important feature of the KNN algorithm is the `n_neighbors` number of neighbors to look at. A too high n will cause overfitting, while a too low number will underfit the model.\n",
    "\n",
    "#### Distance metric\n",
    "The KNN algorithm can choose to use either Manhattan distance or Euclidian distance to calculate how far away the neighbors are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "We will split the data in training and test set, and only use the train data for the cross validation in the grid search. This will reduce the amount of data used to train the models and tuning hyperparameters, butt will ensure no data leakage and help reduce overfitting of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from preprocessor import Preprocessor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score, f1_score, precision_recall_curve, auc, precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = Preprocessor()\n",
    "\n",
    "X_train, X_test, y_train, y_test = prep.get_data(test_size=0.25, impute_method='knn', remove_outliers=True, scaling_method='standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up hyperparameters grid\n",
    "\n",
    "We will explore\n",
    "- `n_neighbors`: odd numbers from 3 to 31.\n",
    "- `p`: Manhattan distance (1) or euclidian distance (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': np.arange(3, 32, 2),  \n",
    "    'p': [1, 2]  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform grid search\n",
    "\n",
    "As stated above, we will focus mainly on recall and F1-score for our model evaluation. For the grid search we will use F1-score as evaluation metric, because\n",
    "- using recall can cause an unacceptably poor precision.\n",
    "- F1-score also considers true positives.\n",
    "\n",
    "To narrow the number of combinations we will further evaluate, we retrieve the six best combinations from the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='f1', return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract the six best iterations, based on mean F1 score\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "top_6 = results.nlargest(6, 'mean_test_score')  \n",
    "# Extract only the interesting columns from the DataFrame\n",
    "top_6 = top_6[['param_n_neighbors', 'param_p', 'mean_test_score', 'std_test_score']]\n",
    "\n",
    "top_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The six best combinations using F1-score are\n",
    "- in the range 9-31 for `n_neighbors`.\n",
    "- all the best are using Manhattan distance.\n",
    "\n",
    "The combination of hyperparameters with the highest mean F1 score from the grid search uses `n_neighbors=13`. Having a relatively low n as the top performer is a good sign, as it avoids overfitting of the model. This combination also has the lowest standard deviation, showing a stable performance across different folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating top 6 combinations\n",
    "We will continue by examining how these six top results perform on unseen data. We will look at \n",
    "- F1-score\n",
    "- Recall\n",
    "- Precision\n",
    "- Precision-Recall Area Under Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Make prediction\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Retrieve scores\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    pr_auc = auc(recall_curve, precision_curve)\n",
    "\n",
    "    return recall, f1, precision, pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame with evaluation metric for each of the top 6 models\n",
    "evaluation_results = pd.DataFrame(columns=['n_neighbors', 'p', 'Recall', 'F1-score', 'Precision', 'Precision-recall AUC'])\n",
    "\n",
    "# Iterate the top 6 parameter combinations\n",
    "for (index, row) in top_6.iterrows():\n",
    "\n",
    "    # Dict with params from the row\n",
    "    params = {\n",
    "        'n_neighbors': row['param_n_neighbors'],\n",
    "        'p': row['param_p']\n",
    "    }\n",
    "\n",
    "    # Fit the model with params\n",
    "    model = KNeighborsClassifier(n_neighbors=params['n_neighbors'], p=params['p'])\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    recall, f1, precision, pr_auc = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    new_row = {\n",
    "        'n_neighbors': params['n_neighbors'],\n",
    "        'p': params['p'],\n",
    "        'Recall': round(recall, 3), \n",
    "        'Precision': round(precision, 3), \n",
    "        'F1-score': round(f1, 3), \n",
    "        'Precision-recall AUC': round(pr_auc, 3)\n",
    "    }\n",
    "    evaluation_results.loc[len(evaluation_results)] = new_row\n",
    "\n",
    "evaluation_results = evaluation_results.sort_values(by=['Recall', 'F1-score'])\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results on the unseen data, we will further evaluate a model using `n_neighbors=9` and `p=1`. This is due to the following considerations:\n",
    "- The Recall rate is tied second best, and only insignificantly lower (0.008) than the best. \n",
    "- The F1-score is tied second best, and only insignificantly lower (0.004) than the best. \n",
    "- While performing approximately as well, this combination has significantly lower n than the top performers (9 compared to 29 and 31). By choosing this, we will get a model that is both less computationally expensive and less prone to overfitting, without compromising the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and variance\n",
    "\n",
    "Next, we examine the model performance in the context of bias and variance by plotting a validation curve for n in the range 1 to 29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter range for validation curve\n",
    "param_range = np.arange(1, 30, 2)\n",
    "\n",
    "# Calculate scores for validation curve\n",
    "train_scores, test_scores = validation_curve(\n",
    "    KNeighborsClassifier(),\n",
    "    X_train, y_train, param_name=\"n_neighbors\", \n",
    "    param_range=param_range,\n",
    "    cv=5, scoring=\"recall\", n_jobs=-1)\n",
    "\n",
    "# Calculate mean and standard deviation for train and test scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the validation curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(param_range, train_mean, label=\"Training score\", color=\"r\")\n",
    "plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"g\")\n",
    "\n",
    "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, color=\"r\", alpha=0.1)\n",
    "plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, color=\"g\", alpha=0.1)\n",
    "\n",
    "plt.title(\"Validation Curve for KNN, recall\")\n",
    "plt.xlabel(\"Number of Neighbors\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation curve shows how recall changes as we adjust `n_neighbors`. We see that\n",
    "\n",
    "- At very low values (close to 1), the recall is extremely high on the training data but much lower on the cross-validation data. This suggests that the model is overfitting, and not generalizing.\n",
    "\n",
    "- As n_neighbors increases, the recall on the training data drops, indicating that the model is starting to generalize. The recall on the cross-validation data improves and stabilizes, which also suggests that the model's ability to generalize is improving.\n",
    "\n",
    "- Between about 9 to 15 neighbors, the training and cross-validation scores are closest, indicating a good balance between learning from the training data and generalizing to new data. This aligns with the choice of `n_neighbors=9`.\n",
    "\n",
    "- Beyond 15, the recall starts to slightly decline or flatten on the cross-validation, which might suggest that further increasing n could cause underfitting.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
